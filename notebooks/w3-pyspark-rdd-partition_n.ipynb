{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/06/20 09:08:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/20 09:08:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"spark-sql\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"/home/jovyan/data/movies.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, movieId: string, title: string, genres: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "|6      |Heat (1995)                       |Action|Crime|Thriller                      |\n",
      "|7      |Sabrina (1995)                    |Comedy|Romance                             |\n",
      "|8      |Tom and Huck (1995)               |Adventure|Children                         |\n",
      "|9      |Sudden Death (1995)               |Action                                     |\n",
      "|10     |GoldenEye (1995)                  |Action|Adventure|Thriller                  |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|genres                                           |\n",
      "+-------------------------------------------------+\n",
      "|Comedy|Horror|Thriller                           |\n",
      "|Adventure|Sci-Fi|Thriller                        |\n",
      "|Action|Adventure|Drama|Fantasy                   |\n",
      "|Action|Drama|Horror                              |\n",
      "|Action|Animation|Comedy|Sci-Fi                   |\n",
      "|Animation|Children|Drama|Musical|Romance         |\n",
      "|Action|Adventure|Drama                           |\n",
      "|Adventure|Sci-Fi                                 |\n",
      "|Documentary|Musical|IMAX                         |\n",
      "|Adventure|Children|Fantasy|Sci-Fi|Thriller       |\n",
      "|Adventure|Animation                              |\n",
      "|Musical|Romance|War                              |\n",
      "|Action|Adventure|Fantasy|Romance                 |\n",
      "|Adventure|Children|Drama|Fantasy|IMAX            |\n",
      "|Comedy|Crime|Horror|Thriller                     |\n",
      "|Crime|Drama|Fantasy|Horror|Thriller              |\n",
      "|Comedy|Mystery|Thriller                          |\n",
      "|Adventure|Fantasy                                |\n",
      "|Horror|Romance|Sci-Fi                            |\n",
      "|Drama|Film-Noir|Romance                          |\n",
      "|Crime|Drama|Film-Noir|Mystery|Thriller           |\n",
      "|Musical|Western                                  |\n",
      "|Adventure|Animation|Children|Musical|Romance     |\n",
      "|Comedy|Crime|Mystery|Romance|Thriller            |\n",
      "|Action|Animation|Children|Crime                  |\n",
      "|Adventure|Horror|Sci-Fi                          |\n",
      "|Comedy|Drama|Romance|Thriller                    |\n",
      "|Action|Drama|Mystery|Sci-Fi|Thriller             |\n",
      "|Adventure|Animation|Children|Comedy|Drama|Fantasy|\n",
      "|Action|Adventure|Mystery|Sci-Fi                  |\n",
      "+-------------------------------------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_df = spark.sql(\"SELECT DISTINCT(genres) FROM movies\")\n",
    "n_df.show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|title                                     |\n",
      "+------------------------------------------+\n",
      "|Bottle Rocket (1996)                      |\n",
      "|Mr. Wrong (1996)                          |\n",
      "|Unforgettable (1996)                      |\n",
      "|Happy Gilmore (1996)                      |\n",
      "|Bridges of Madison County, The (1995)     |\n",
      "|Nobody Loves Me (Keiner liebt mich) (1994)|\n",
      "|Muppet Treasure Island (1996)             |\n",
      "|Catwalk (1996)                            |\n",
      "|Braveheart (1995)                         |\n",
      "|Taxi Driver (1976)                        |\n",
      "+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT title FROM movies WHERE movieId > 100\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9732"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT movieId, genres\n",
    "    FROM movies\n",
    "    GROUP BY genres, movieId\"\"\"\n",
    "    ).where(\"movieId > 10\"\n",
    "    ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD with partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"./data/movies.csv\")\n",
    "rdd2 = sc.textFile(\"./data/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultMinPartitions: 1\n",
      "name: rdd1, count: <bound method RDD.count of ./data/movies.csv MapPartitionsRDD[46] at textFile at NativeMethodAccessorImpl.java:0>, partitions: 1\n",
      "name: rdd2, count: <bound method RDD.count of ./data/ratings.csv MapPartitionsRDD[48] at textFile at NativeMethodAccessorImpl.java:0>, partitions: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"defaultMinPartitions: {sc.defaultMinPartitions}\")\n",
    "print(f\"name: rdd1, count: {rdd1.count}, partitions: {rdd1.getNumPartitions()}\")\n",
    "print(f\"name: rdd2, count: {rdd2.count}, partitions: {rdd2.getNumPartitions()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE 연봉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "|company_location|company_size|employee_residence|employment_type|experience_level|job_title                     |remote_ratio|salary|salary_currency|salary_in_usd|work_year|\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "|US              |M           |US                |FT             |EX              |Data Science Director         |0           |212000|USD            |212000       |2023     |\n",
      "|US              |M           |US                |FT             |EX              |Data Science Director         |0           |190000|USD            |190000       |2023     |\n",
      "|GB              |M           |GB                |FT             |MI              |Business Intelligence Engineer|0           |35000 |GBP            |43064        |2023     |\n",
      "|GB              |M           |GB                |FT             |MI              |Business Intelligence Engineer|0           |35000 |GBP            |43064        |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |245700|USD            |245700       |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |132300|USD            |132300       |2023     |\n",
      "|US              |M           |US                |FT             |MI              |Data Specialist               |0           |90000 |USD            |90000        |2023     |\n",
      "|US              |M           |US                |FT             |MI              |Data Specialist               |0           |80000 |USD            |80000        |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |212000|USD            |212000       |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |93300 |USD            |93300        |2023     |\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# JSON 데이터의 스키마 정의\n",
    "salaries_schema = StructType([\n",
    "    StructField('company_location', StringType(), True),\n",
    "    StructField('company_size', StringType(), True),\n",
    "    StructField('employee_residence', StringType(), True),\n",
    "    StructField('employment_type', StringType(), True),\n",
    "    StructField('experience_level', StringType(), True),\n",
    "    StructField('job_title', StringType(), True),\n",
    "    StructField('remote_ratio', IntegerType(), True),\n",
    "    StructField('salary', StringType(), True),  # Initially as StringType\n",
    "    StructField('salary_currency', StringType(), True),\n",
    "    StructField('salary_in_usd', StringType(), True),  # Initially as StringType\n",
    "    StructField('work_year', StringType(), True)\n",
    "])\n",
    "\n",
    "# JSON 파일 읽기\n",
    "players_df = spark.read.schema(player_schema).json('path/to/your/players_data.json')\n",
    "\n",
    "# DataFrame의 상위 10개 행을 표시\n",
    "players_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- remote_ratio: long (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: string (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      "\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "|company_location|company_size|employee_residence|employment_type|experience_level|job_title                     |remote_ratio|salary|salary_currency|salary_in_usd|work_year|\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "|US              |M           |US                |FT             |EX              |Data Science Director         |0           |212000|USD            |212000       |2023     |\n",
      "|US              |M           |US                |FT             |EX              |Data Science Director         |0           |190000|USD            |190000       |2023     |\n",
      "|GB              |M           |GB                |FT             |MI              |Business Intelligence Engineer|0           |35000 |GBP            |43064        |2023     |\n",
      "|GB              |M           |GB                |FT             |MI              |Business Intelligence Engineer|0           |35000 |GBP            |43064        |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |245700|USD            |245700       |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |132300|USD            |132300       |2023     |\n",
      "|US              |M           |US                |FT             |MI              |Data Specialist               |0           |90000 |USD            |90000        |2023     |\n",
      "|US              |M           |US                |FT             |MI              |Data Specialist               |0           |80000 |USD            |80000        |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |212000|USD            |212000       |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |93300 |USD            |93300        |2023     |\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 자동 추론\n",
    "raw_df = spark.read.json('./data/salaries.json')\n",
    "\n",
    "# 상위 10개 행을 출력\n",
    "raw_df.printSchema()\n",
    "raw_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쇼핑몰 리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|Id    |NickName        |\n",
      "+------+----------------+\n",
      "|103603|1000kgthanh     |\n",
      "|103760|999999999ok     |\n",
      "|103829|ac7ive          |\n",
      "|1     |admin           |\n",
      "|103839|ahkk.nguyen     |\n",
      "|103981|akyshin         |\n",
      "|103863|alex.tran.7792  |\n",
      "|103694|ali33va40tencuop|\n",
      "|103547|alice.mi.39     |\n",
      "|103523|all4u4me        |\n",
      "+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 고객 데이터 스키마 정의\n",
    "customers_schema = StructType([\n",
    "    StructField('Id', IntegerType(), True),\n",
    "    StructField('NickName', StringType(), True)\n",
    "])\n",
    "\n",
    "# 고객 데이터 로드\n",
    "customers_df = spark.read.schema(customers_schema).json('./data/customers.json')\n",
    "\n",
    "# 데이터프레임 확인\n",
    "customers_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------+---------+\n",
      "|Id |Name                                          |UnitPrice|\n",
      "+---+----------------------------------------------+---------+\n",
      "|1  |Build your own computer                       |1200     |\n",
      "|2  |Digital Storm VANQUISH 3 Custom Performance PC|1259     |\n",
      "|3  |Lenovo IdeaCentre 600 All-in-One PC           |500      |\n",
      "|4  |Apple MacBook Pro 13-inch                     |1800     |\n",
      "|5  |Asus N551JK-XO076H Laptop                     |1500     |\n",
      "|6  |Samsung Series 9 NP900X4C Premium Ultrabook   |1590     |\n",
      "|7  |HP Spectre XT Pro UltraBook                   |1350     |\n",
      "|8  |HP Envy 6-1180ca 15.6-Inch Sleekbook          |1460     |\n",
      "|9  |Lenovo Thinkpad X1 Carbon Laptop              |1360     |\n",
      "|10 |Adobe Photoshop CS4                           |75       |\n",
      "+---+----------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 제품 데이터 스키마 정의\n",
    "products_schema = StructType([\n",
    "    StructField('Id', IntegerType(), True),\n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('UnitPrice', StringType(), True)\n",
    "])\n",
    "\n",
    "# 제품 데이터 로드\n",
    "products_df = spark.read.schema(products_schema).json('./data/products.json')\n",
    "\n",
    "# 데이터프레임 확인\n",
    "products_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+-------------------+\n",
      "|CustomerID|ProductID|Rate|CreateDate         |\n",
      "+----------+---------+----+-------------------+\n",
      "|103416    |619      |1   |2018/01/01 01:36:30|\n",
      "|103654    |411      |1   |2018/01/01 01:36:35|\n",
      "|103954    |298      |3   |2018/01/01 01:36:38|\n",
      "|103672    |361      |5   |2018/01/01 01:37:15|\n",
      "|103960    |536      |5   |2018/01/01 02:36:25|\n",
      "|103372    |481      |2   |2018/01/01 02:36:32|\n",
      "|103444    |132      |1   |2018/01/01 02:36:34|\n",
      "|103831    |41       |1   |2018/01/01 02:36:41|\n",
      "|103541    |498      |5   |2018/01/01 02:36:50|\n",
      "|103819    |155      |4   |2018/01/01 02:37:10|\n",
      "+----------+---------+----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 평가 데이터 스키마 정의\n",
    "ratings_schema = StructType([\n",
    "    StructField('CustomerID', IntegerType(), True),\n",
    "    StructField('ProductID', IntegerType(), True),\n",
    "    StructField('Rate', IntegerType(), True),\n",
    "    StructField('CreateDate', StringType(), True)\n",
    "])\n",
    "\n",
    "# 평가 데이터 로드\n",
    "ratings_df = spark.read.schema(ratings_schema).json('./data/new_ratings.json')  # 올바른 파일 경로로 수정 필요\n",
    "\n",
    "# 데이터프레임 확인\n",
    "ratings_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|CustomerID|ProductID|Rate|CreateDate         |NickName        |Name                                                                                                                       |UnitPrice|\n",
      "+----------+---------+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|103416    |619      |1   |2018/01/01 01:36:30|vanduong0403    |Le Vian Chocolate Diamonds 1/4 ct tw Earrings 14K Honey Gold                                                               |1049     |\n",
      "|103654    |411      |1   |2018/01/01 01:36:35|thutruc.huynh.3 |Diamond Heart Necklace 1/10 ct tw Round-Cut 10K Yellow Gold 18\"                                                            |299.99   |\n",
      "|103954    |298      |3   |2018/01/01 01:36:38|ha.n.hien.75    |Heart Ring 1/5 ct tw Diamonds 10K Two-Tone Gold                                                                            |479      |\n",
      "|103672    |361      |5   |2018/01/01 01:37:15|nam.kimcham.1   |Neil Lane Diamond Wedding Band 1/4 ct tw 14K White Gold                                                                    |719.99   |\n",
      "|103960    |536      |5   |2018/01/01 02:36:25|tuanpkna        |Disney Treasures Winnie the Pooh Mother of Pearl & Diamond Heart Necklace 1/8 ct tw Sterling Silver & 10K Two-Tone Gold 17\"|449.99   |\n",
      "|103372    |481      |2   |2018/01/01 02:36:32|damphuhanh      |Amethyst Heart Necklace Tanzanites & Diamonds 10K White Gold                                                               |599.99   |\n",
      "|103444    |132      |1   |2018/01/01 02:36:34|Kukumalu.Thu    |Chân Váy B&Y                                                                                                               |7        |\n",
      "|103831    |41       |1   |2018/01/01 02:36:41|hip01           |Flower Girl Bracelet                                                                                                       |360      |\n",
      "|103541    |498      |5   |2018/01/01 02:36:50|kitty.ngo.5     |Citrine & Diamond Necklace Sterling Silver                                                                                 |179.99   |\n",
      "|103819    |155      |4   |2018/01/01 02:37:10|tran.anh.khoa.86|Túi mây hình thang                                                                                                         |24       |\n",
      "+----------+---------+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 고객 데이터와 평가 데이터 병합\n",
    "merged_df = ratings_df.join(customers_df, ratings_df.CustomerID == customers_df.Id, 'inner') \\\n",
    "                      .select(ratings_df['*'], customers_df['NickName'])\n",
    "\n",
    "# 병합된 데이터와 제품 데이터 병합\n",
    "final_df = merged_df.join(products_df, merged_df.ProductID == products_df.Id, 'inner') \\\n",
    "                    .select(merged_df['*'], products_df['Name'], products_df['UnitPrice'])\n",
    "\n",
    "# 최종 데이터프레임 확인\n",
    "final_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========각 고객별 평가한 제품의 평균 평점===========\n",
      "+------------------+------------------+\n",
      "|NickName          |AverageRating     |\n",
      "+------------------+------------------+\n",
      "|chausoco          |4.0               |\n",
      "|Kojumi            |5.0               |\n",
      "|trannhatvy        |5.0               |\n",
      "|ruud0407          |2.5081521739130435|\n",
      "|lenam.uit         |4.006306937631394 |\n",
      "|thanh.cule        |5.0               |\n",
      "|nguyettram        |3.375912408759124 |\n",
      "|NguyenVuNhatChuong|4.0               |\n",
      "|ptkiuee           |4.0               |\n",
      "|tuonglam.dang     |3.6470588235294117|\n",
      "+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 고객이 평가한 제품의 평균 평점 계산\n",
    "average_ratings = final_df.groupBy('NickName').agg(F.avg('Rate').alias('AverageRating'))\n",
    "\n",
    "print('===========각 고객별 평가한 제품의 평균 평점===========')\n",
    "\n",
    "# 결과 확인\n",
    "average_ratings.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수업 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------------------------------+--------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+\n",
      "|login              |url                                             |created_at          |id         |repository_id|size|distinct_size|message|type       |name                                    |url                                                                  |\n",
      "+-------------------+------------------------------------------------+--------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+\n",
      "|Drunkula           |https://api.github.com/users/Drunkula           |2024-05-19T14:00:00Z|38509489462|489717552    |1   |1            |null   |PushEvent  |Drunkula/twitchtoolsglitch              |https://api.github.com/repos/Drunkula/twitchtoolsglitch              |\n",
      "|allenlooplee       |https://api.github.com/users/allenlooplee       |2024-05-19T14:00:00Z|38509489467|791822148    |1   |1            |null   |PushEvent  |allenlooplee/open-piggy-ios             |https://api.github.com/repos/allenlooplee/open-piggy-ios             |\n",
      "|carmithersh        |https://api.github.com/users/carmithersh        |2024-05-19T14:00:00Z|38509489469|802788764    |1   |1            |null   |PushEvent  |carmithersh/setup-jfrog-cli             |https://api.github.com/repos/carmithersh/setup-jfrog-cli             |\n",
      "|github-actions[bot]|https://api.github.com/users/github-actions[bot]|2024-05-19T14:00:00Z|38509489476|400840924    |1   |1            |null   |PushEvent  |Christophecs2/Christophecs2             |https://api.github.com/repos/Christophecs2/Christophecs2             |\n",
      "|github-actions[bot]|https://api.github.com/users/github-actions[bot]|2024-05-19T14:00:00Z|38509489485|492467823    |1   |1            |null   |PushEvent  |itz-Amethyst/itz-Amethyst               |https://api.github.com/repos/itz-Amethyst/itz-Amethyst               |\n",
      "|maksimgu90         |https://api.github.com/users/maksimgu90         |2024-05-19T14:00:00Z|38509489490|null         |null|null         |null   |PublicEvent|maksimgu90/cpp-search-server            |https://api.github.com/repos/maksimgu90/cpp-search-server            |\n",
      "|tipo23             |https://api.github.com/users/tipo23             |2024-05-19T14:00:00Z|38509489497|730932158    |1   |1            |null   |PushEvent  |tipo23/dark-web-links                   |https://api.github.com/repos/tipo23/dark-web-links                   |\n",
      "|rlmgtr             |https://api.github.com/users/rlmgtr             |2024-05-19T14:00:00Z|38509489499|null         |null|null         |null   |CreateEvent|rlmgtr/javascript-teamProject-weatherApp|https://api.github.com/repos/rlmgtr/javascript-teamProject-weatherApp|\n",
      "|Mix-Link           |https://api.github.com/users/Mix-Link           |2024-05-19T14:00:00Z|38509489502|795074361    |1   |1            |null   |PushEvent  |Mix-Link/darkweb                        |https://api.github.com/repos/Mix-Link/darkweb                        |\n",
      "|pingowl            |https://api.github.com/users/pingowl            |2024-05-19T14:00:00Z|38509489505|662925679    |1   |1            |null   |PushEvent  |pingowl/Algorithm                       |https://api.github.com/repos/pingowl/Algorithm                       |\n",
      "+-------------------+------------------------------------------------+--------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType #사용할 칼럼을 잡아서 설정\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "actor_schema = StructType([\n",
    "    StructField('login', StringType(), True),\n",
    "    StructField('url', StringType(), True)\n",
    "])\n",
    "\n",
    "payload_schema = StructType([\n",
    "    StructField('repository_id', LongType(), True),\n",
    "    StructField('size', LongType(), True),\n",
    "    StructField('distinct_size', LongType(), True),\n",
    "    StructField('message', StringType(), True)\n",
    "])\n",
    "\n",
    "repo_schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('url', StringType(), True)\n",
    "])\n",
    "\n",
    "new_df = github.withColumn('actor_json', F.from_json('actor', actor_schema)) \\\n",
    "               .select('created_at', 'id', 'payload', 'type', 'actor_json.*', 'repo')\n",
    "new_df = new_df.withColumn('payload_json', F.from_json('payload', payload_schema)) \\\n",
    "               .select('login', 'url', 'created_at', 'id', 'payload_json.*', 'type', 'repo')\n",
    "new_df = new_df.withColumn('repo_json', F.from_json('repo', repo_schema)) \\\n",
    "               .select('login', 'url', 'created_at', 'id', 'repository_id', 'size', 'distinct_size', 'message', 'type', 'repo_json.*')\n",
    "new_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|login     |url    |created_at    |id   |repository_id|size|distinct_size|message|type  |\n",
    "| --- | ---| ---| --- | --- | --- | --- | --- | --- |\n",
    "| String | String | DateTime | Long | Long | Int | Int | String | String |\n",
    "\n",
    "Top 50 Push repositories\n",
    "Top 50 Commit repositories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샐러리 & 쇼핑 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- Rate: integer (nullable = true)\n",
      " |-- CreateDate: string (nullable = true)\n",
      " |-- NickName: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      "\n",
      "=========================================================\n",
      "root\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: long (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()\n",
    "print(\"=========================================================\")\n",
    "salaries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|CustomerID|ProductID|Rate|CreateDate         |NickName        |Name                                                                                                                       |UnitPrice|\n",
      "+----------+---------+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|103416    |619      |1   |2018/01/01 01:36:30|vanduong0403    |Le Vian Chocolate Diamonds 1/4 ct tw Earrings 14K Honey Gold                                                               |1049     |\n",
      "|103654    |411      |1   |2018/01/01 01:36:35|thutruc.huynh.3 |Diamond Heart Necklace 1/10 ct tw Round-Cut 10K Yellow Gold 18\"                                                            |299.99   |\n",
      "|103954    |298      |3   |2018/01/01 01:36:38|ha.n.hien.75    |Heart Ring 1/5 ct tw Diamonds 10K Two-Tone Gold                                                                            |479      |\n",
      "|103672    |361      |5   |2018/01/01 01:37:15|nam.kimcham.1   |Neil Lane Diamond Wedding Band 1/4 ct tw 14K White Gold                                                                    |719.99   |\n",
      "|103960    |536      |5   |2018/01/01 02:36:25|tuanpkna        |Disney Treasures Winnie the Pooh Mother of Pearl & Diamond Heart Necklace 1/8 ct tw Sterling Silver & 10K Two-Tone Gold 17\"|449.99   |\n",
      "|103372    |481      |2   |2018/01/01 02:36:32|damphuhanh      |Amethyst Heart Necklace Tanzanites & Diamonds 10K White Gold                                                               |599.99   |\n",
      "|103444    |132      |1   |2018/01/01 02:36:34|Kukumalu.Thu    |Chân Váy B&Y                                                                                                               |7        |\n",
      "|103831    |41       |1   |2018/01/01 02:36:41|hip01           |Flower Girl Bracelet                                                                                                       |360      |\n",
      "|103541    |498      |5   |2018/01/01 02:36:50|kitty.ngo.5     |Citrine & Diamond Necklace Sterling Silver                                                                                 |179.99   |\n",
      "|103819    |155      |4   |2018/01/01 02:37:10|tran.anh.khoa.86|Túi mây hình thang                                                                                                         |24       |\n",
      "+----------+---------+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "=========================================================\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "|company_location|company_size|employee_residence|employment_type|experience_level|job_title                     |remote_ratio|salary|salary_currency|salary_in_usd|work_year|\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "|US              |M           |US                |FT             |EX              |Data Science Director         |0           |212000|USD            |212000       |2023     |\n",
      "|US              |M           |US                |FT             |EX              |Data Science Director         |0           |190000|USD            |190000       |2023     |\n",
      "|GB              |M           |GB                |FT             |MI              |Business Intelligence Engineer|0           |35000 |GBP            |43064        |2023     |\n",
      "|GB              |M           |GB                |FT             |MI              |Business Intelligence Engineer|0           |35000 |GBP            |43064        |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |245700|USD            |245700       |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |132300|USD            |132300       |2023     |\n",
      "|US              |M           |US                |FT             |MI              |Data Specialist               |0           |90000 |USD            |90000        |2023     |\n",
      "|US              |M           |US                |FT             |MI              |Data Specialist               |0           |80000 |USD            |80000        |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |212000|USD            |212000       |2023     |\n",
      "|US              |M           |US                |FT             |SE              |Machine Learning Engineer     |0           |93300 |USD            |93300        |2023     |\n",
      "+----------------+------------+------------------+---------------+----------------+------------------------------+------------+------+---------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(10,False)\n",
    "print(\"=========================================================\")\n",
    "salaries_df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수업 예시 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+-------------------+\n",
      "|login       |url                                      |created_at         |id         |repository_id|size|distinct_size|message|type       |name                                    |url                                                                  |created_dt         |\n",
      "+------------+-----------------------------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+-------------------+\n",
      "|Drunkula    |https://api.github.com/users/Drunkula    |2024-05-19 14:00:00|38509489462|489717552    |1   |1            |null   |PushEvent  |Drunkula/twitchtoolsglitch              |https://api.github.com/repos/Drunkula/twitchtoolsglitch              |2024-05-19 14:00:00|\n",
      "|allenlooplee|https://api.github.com/users/allenlooplee|2024-05-19 14:00:00|38509489467|791822148    |1   |1            |null   |PushEvent  |allenlooplee/open-piggy-ios             |https://api.github.com/repos/allenlooplee/open-piggy-ios             |2024-05-19 14:00:00|\n",
      "|carmithersh |https://api.github.com/users/carmithersh |2024-05-19 14:00:00|38509489469|802788764    |1   |1            |null   |PushEvent  |carmithersh/setup-jfrog-cli             |https://api.github.com/repos/carmithersh/setup-jfrog-cli             |2024-05-19 14:00:00|\n",
      "|maksimgu90  |https://api.github.com/users/maksimgu90  |2024-05-19 14:00:00|38509489490|null         |null|null         |null   |PublicEvent|maksimgu90/cpp-search-server            |https://api.github.com/repos/maksimgu90/cpp-search-server            |2024-05-19 14:00:00|\n",
      "|tipo23      |https://api.github.com/users/tipo23      |2024-05-19 14:00:00|38509489497|730932158    |1   |1            |null   |PushEvent  |tipo23/dark-web-links                   |https://api.github.com/repos/tipo23/dark-web-links                   |2024-05-19 14:00:00|\n",
      "|rlmgtr      |https://api.github.com/users/rlmgtr      |2024-05-19 14:00:00|38509489499|null         |null|null         |null   |CreateEvent|rlmgtr/javascript-teamProject-weatherApp|https://api.github.com/repos/rlmgtr/javascript-teamProject-weatherApp|2024-05-19 14:00:00|\n",
      "|Mix-Link    |https://api.github.com/users/Mix-Link    |2024-05-19 14:00:00|38509489502|795074361    |1   |1            |null   |PushEvent  |Mix-Link/darkweb                        |https://api.github.com/repos/Mix-Link/darkweb                        |2024-05-19 14:00:00|\n",
      "|pingowl     |https://api.github.com/users/pingowl     |2024-05-19 14:00:00|38509489505|662925679    |1   |1            |null   |PushEvent  |pingowl/Algorithm                       |https://api.github.com/repos/pingowl/Algorithm                       |2024-05-19 14:00:00|\n",
      "|Artheek18   |https://api.github.com/users/Artheek18   |2024-05-19 14:00:00|38509489508|802567839    |1   |1            |null   |PushEvent  |Artheek18/HawkHacks                     |https://api.github.com/repos/Artheek18/HawkHacks                     |2024-05-19 14:00:00|\n",
      "|Lanvry      |https://api.github.com/users/Lanvry      |2024-05-19 14:00:00|38509489510|null         |null|null         |null   |CreateEvent|Lanvry/APIEndpoint                      |https://api.github.com/repos/Lanvry/APIEndpoint                      |2024-05-19 14:00:00|\n",
      "+------------+-----------------------------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter github action bot\n",
    "new_df = new_df.filter(col(\"login\") != \"github-actions[bot]\")\n",
    "new_df = new_df.withColumn('created_at', F.trim(F.regexp_replace(new_df.created_at, \"[TZ]\", \" \")))\n",
    "new_df = new_df.withColumn('created_dt', F.to_timestamp(new_df.created_at, 'yyyy-MM-dd HH:mm:ss'))\n",
    "new_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- login: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- repository_id: long (nullable = true)\n",
      " |-- size: long (nullable = true)\n",
      " |-- distinct_size: long (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- created_dt: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repo_name(name):\n",
    "    sp = name.split(\"/\")\n",
    "    if not sp:\n",
    "        return name\n",
    "    else:\n",
    "        return sp[-1]\n",
    "    \n",
    "udf_check_repo_name = F.udf(check_repo_name, StringType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @F.udf(returnType=StringType())\n",
    "# def check_repo_name(val):\n",
    "#     sp = name.split(\"/\")\n",
    "#     if not sp:\n",
    "#         return name\n",
    "#     else:\n",
    "#         return sp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.udf(lambda name => name.split(\"/\")[-1], StringType()) 펑션 만드는 방법 2개 가져옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+-------------------+---------------------------------+\n",
      "|login       |url                                      |created_at         |id         |repository_id|size|distinct_size|message|type       |name                                    |url                                                                  |created_dt         |repo_name                        |\n",
      "+------------+-----------------------------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+-------------------+---------------------------------+\n",
      "|Drunkula    |https://api.github.com/users/Drunkula    |2024-05-19 14:00:00|38509489462|489717552    |1   |1            |null   |PushEvent  |Drunkula/twitchtoolsglitch              |https://api.github.com/repos/Drunkula/twitchtoolsglitch              |2024-05-19 14:00:00|twitchtoolsglitch                |\n",
      "|allenlooplee|https://api.github.com/users/allenlooplee|2024-05-19 14:00:00|38509489467|791822148    |1   |1            |null   |PushEvent  |allenlooplee/open-piggy-ios             |https://api.github.com/repos/allenlooplee/open-piggy-ios             |2024-05-19 14:00:00|open-piggy-ios                   |\n",
      "|carmithersh |https://api.github.com/users/carmithersh |2024-05-19 14:00:00|38509489469|802788764    |1   |1            |null   |PushEvent  |carmithersh/setup-jfrog-cli             |https://api.github.com/repos/carmithersh/setup-jfrog-cli             |2024-05-19 14:00:00|setup-jfrog-cli                  |\n",
      "|maksimgu90  |https://api.github.com/users/maksimgu90  |2024-05-19 14:00:00|38509489490|null         |null|null         |null   |PublicEvent|maksimgu90/cpp-search-server            |https://api.github.com/repos/maksimgu90/cpp-search-server            |2024-05-19 14:00:00|cpp-search-server                |\n",
      "|tipo23      |https://api.github.com/users/tipo23      |2024-05-19 14:00:00|38509489497|730932158    |1   |1            |null   |PushEvent  |tipo23/dark-web-links                   |https://api.github.com/repos/tipo23/dark-web-links                   |2024-05-19 14:00:00|dark-web-links                   |\n",
      "|rlmgtr      |https://api.github.com/users/rlmgtr      |2024-05-19 14:00:00|38509489499|null         |null|null         |null   |CreateEvent|rlmgtr/javascript-teamProject-weatherApp|https://api.github.com/repos/rlmgtr/javascript-teamProject-weatherApp|2024-05-19 14:00:00|javascript-teamProject-weatherApp|\n",
      "|Mix-Link    |https://api.github.com/users/Mix-Link    |2024-05-19 14:00:00|38509489502|795074361    |1   |1            |null   |PushEvent  |Mix-Link/darkweb                        |https://api.github.com/repos/Mix-Link/darkweb                        |2024-05-19 14:00:00|darkweb                          |\n",
      "|pingowl     |https://api.github.com/users/pingowl     |2024-05-19 14:00:00|38509489505|662925679    |1   |1            |null   |PushEvent  |pingowl/Algorithm                       |https://api.github.com/repos/pingowl/Algorithm                       |2024-05-19 14:00:00|Algorithm                        |\n",
      "|Artheek18   |https://api.github.com/users/Artheek18   |2024-05-19 14:00:00|38509489508|802567839    |1   |1            |null   |PushEvent  |Artheek18/HawkHacks                     |https://api.github.com/repos/Artheek18/HawkHacks                     |2024-05-19 14:00:00|HawkHacks                        |\n",
      "|Lanvry      |https://api.github.com/users/Lanvry      |2024-05-19 14:00:00|38509489510|null         |null|null         |null   |CreateEvent|Lanvry/APIEndpoint                      |https://api.github.com/repos/Lanvry/APIEndpoint                      |2024-05-19 14:00:00|APIEndpoint                      |\n",
      "+------------+-----------------------------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+----------------------------------------+---------------------------------------------------------------------+-------------------+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "new_df = new_df.withColumn('repo_name', udf_check_repo_name(F.col('name')))\n",
    "new_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "171662"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130754"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8805"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaries_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(repo_name)|\n",
      "+----------------+\n",
      "|50439           |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_df.agg(F.countDistinct('repo_name')).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(NickName)|\n",
      "+---------------+\n",
      "|344            |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.agg(F.countDistinct('NickName')).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(company_location)|\n",
      "+-----------------------+\n",
      "|74                     |\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.agg(F.countDistinct('company_location')).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50439"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Window\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "w = Window.partitionBy(\"repo_name\").orderBy(F.desc(col(\"size\")))\n",
    "new_df.withColumn(\"row\", row_number().over(w)) \\\n",
    "      .filter(col(\"row\") == 1).drop(\"row\") \\\n",
    "      .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model\n",
    "\n",
    "1. Top 10 Repo\n",
    "- id\n",
    "- @timestamp\n",
    "- repo_url\n",
    "- repo_name\n",
    "- push_count\n",
    "- commit_count\n",
    "- pr_count\n",
    "- fork_count\n",
    "- issue_count\n",
    "- watch_count\n",
    "\n",
    "2. Top 10 User\n",
    "- id\n",
    "- @timestamp\n",
    "- user_name\n",
    "- push_count\n",
    "- commit_count\n",
    "- pr_count\n",
    "- issue_count\n",
    "- issue_comment_count\n",
    "\n",
    "3. Daily Stats\n",
    "- id\n",
    "- @timestamp\n",
    "- distinct_user_cnt\n",
    "- distinct_repo_cnt\n",
    "- push_count\n",
    "- commit_count\n",
    "- pr_count\n",
    "- issue_count\n",
    "- issue_comment_count\n",
    "- release_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           job_title|    average_salary|\n",
      "+--------------------+------------------+\n",
      "|Business Intellig...|       126253.1875|\n",
      "|  Lead Data Engineer|139230.33333333334|\n",
      "|        AI Architect|          250328.0|\n",
      "|        Data Modeler|          128400.2|\n",
      "|Data Visualizatio...|          122275.0|\n",
      "| Data Scientist Lead|          136153.0|\n",
      "|  Decision Scientist|166094.63157894736|\n",
      "|Principal Data Ar...|           38154.0|\n",
      "|Head of Machine L...|          198103.0|\n",
      "|Machine Learning ...|188440.26666666666|\n",
      "|Data Analytics Sp...|           95000.0|\n",
      "|     Data Specialist|          103102.5|\n",
      "|  Sales Data Analyst|           60000.0|\n",
      "|Data Operations E...|         133431.25|\n",
      "|Data Science Prac...|          144480.0|\n",
      "|Data Operations M...|          136000.0|\n",
      "| Data Analytics Lead|         162533.75|\n",
      "|  Power BI Developer|           64781.0|\n",
      "|Deep Learning Res...|          124163.0|\n",
      "|Consultant Data E...|          118539.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+------------------+\n",
      "|experience_level|    average_salary|\n",
      "+----------------+------------------+\n",
      "|              EX|189052.61710037175|\n",
      "|              MI| 114681.0213625866|\n",
      "|              EN| 87676.76282051283|\n",
      "|              SE|161889.01057449495|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "salaries_df.groupBy(\"job_title\").agg(avg(\"salary_in_usd\").alias(\"average_salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|experience_level|    average_salary|\n",
      "+----------------+------------------+\n",
      "|              EX|189052.61710037175|\n",
      "|              MI| 114681.0213625866|\n",
      "|              EN| 87676.76282051283|\n",
      "|              SE|161889.01057449495|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "salaries_df.groupBy(\"experience_level\").agg(avg(\"salary_in_usd\").alias(\"average_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|company_location|    average_salary|\n",
      "+----------------+------------------+\n",
      "|              LT|           97611.0|\n",
      "|              DZ|          100000.0|\n",
      "|              FI| 68519.33333333333|\n",
      "|              UA|121333.33333333333|\n",
      "|              RO|50950.666666666664|\n",
      "|              NL|           77956.2|\n",
      "|              BS|           45555.0|\n",
      "|              PL|          53923.75|\n",
      "|              AM|           50000.0|\n",
      "|              MX| 94864.63636363637|\n",
      "|              EE|           46706.1|\n",
      "|              CN|          100000.0|\n",
      "|              AT| 71354.83333333333|\n",
      "|              RU| 78207.85714285714|\n",
      "|              IQ|          100000.0|\n",
      "|              AD|           50745.0|\n",
      "|              HR|           76726.0|\n",
      "|              CZ| 69478.66666666667|\n",
      "|              PT|49787.083333333336|\n",
      "|              GH|           27000.0|\n",
      "+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.groupBy(\"company_location\").agg(avg(\"salary_in_usd\").alias(\"average_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|remote_ratio|    average_salary|\n",
      "+------------+------------------+\n",
      "|         100|144149.24196482718|\n",
      "|          50| 82162.11926605504|\n",
      "|           0|155592.48194365663|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.groupBy(\"remote_ratio\").agg(avg(\"salary_in_usd\").alias(\"average_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                Name|    average_rating|\n",
      "+--------------------+------------------+\n",
      "|Diamond Necklace ...|3.1396648044692737|\n",
      "|     Dip Dye Sweater|3.2151162790697674|\n",
      "|Le Vian Diamond R...|3.2032967032967035|\n",
      "|Converse đen cổ thấp| 3.159779614325069|\n",
      "|Flower Earrings 1...|3.1604938271604937|\n",
      "|Disney Treasures ...|3.3248730964467006|\n",
      "|  Vải Thiều Thanh Hà|3.2763819095477387|\n",
      "|Hoop Earrings 14K...| 3.143617021276596|\n",
      "|Unstoppable Love ...| 3.277511961722488|\n",
      "|The Gangster, The...|3.0757575757575757|\n",
      "|     Science & Faith|3.2254901960784315|\n",
      "|         Xiaomi Mi 8|3.1707317073170733|\n",
      "|Hallmark Diamonds...|3.1129943502824857|\n",
      "|    Paper Coffee Cup|3.1837837837837837|\n",
      "|    Levi's 511 Jeans| 3.263157894736842|\n",
      "|Basic Jogger In B...|3.1508379888268156|\n",
      "|Disney Treasures ...|3.1510416666666665|\n",
      "|Le Bleu Homestay ...| 3.020942408376963|\n",
      "|Samsung Galaxy Z ...| 3.098360655737705|\n",
      "|Diamond Heart Nec...| 3.187192118226601|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.groupBy(\"Name\").agg(F.avg(\"Rate\").alias(\"average_rating\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|ProductID|review_count|\n",
      "+---------+------------+\n",
      "|      148|         186|\n",
      "|      471|         179|\n",
      "|      496|         176|\n",
      "|      463|         177|\n",
      "|      540|         184|\n",
      "|      243|         192|\n",
      "|      392|         189|\n",
      "|      623|         176|\n",
      "|       31|         184|\n",
      "|      516|         201|\n",
      "|      137|         193|\n",
      "|      451|         201|\n",
      "|      251|         198|\n",
      "|       85|         192|\n",
      "|      580|         199|\n",
      "|      458|         184|\n",
      "|       65|         167|\n",
      "|      481|         199|\n",
      "|      588|         204|\n",
      "|      255|         203|\n",
      "+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/19 17:08:21 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2931050 ms exceeds timeout 120000 ms\n",
      "24/06/19 17:08:21 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "final_df.groupBy(\"ProductID\").agg(F.count(\"Rate\").alias(\"review_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                type|\n",
      "+--------------------+\n",
      "|PullRequestReview...|\n",
      "|           PushEvent|\n",
      "|         GollumEvent|\n",
      "|        ReleaseEvent|\n",
      "|  CommitCommentEvent|\n",
      "|         CreateEvent|\n",
      "|PullRequestReview...|\n",
      "|   IssueCommentEvent|\n",
      "|         DeleteEvent|\n",
      "|         IssuesEvent|\n",
      "|           ForkEvent|\n",
      "|         PublicEvent|\n",
      "|         MemberEvent|\n",
      "|          WatchEvent|\n",
      "|    PullRequestEvent|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(\"type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|    average_rating|\n",
      "+----------+------------------+\n",
      "|    103747|               4.0|\n",
      "|    103638|               3.0|\n",
      "|    103767|               5.0|\n",
      "|    103988|               5.0|\n",
      "|    103713|               1.0|\n",
      "|    103541| 3.821852731591449|\n",
      "|    103970|               3.0|\n",
      "|    103506| 3.699099099099099|\n",
      "|    103883| 4.371373307543521|\n",
      "|    103880|2.3123359580052494|\n",
      "|    103962|2.9343629343629343|\n",
      "|    103648|               5.0|\n",
      "|    103755|               2.0|\n",
      "|     15221|               1.0|\n",
      "|    103476|2.5081521739130435|\n",
      "|    103698|               1.0|\n",
      "|    103876|               4.0|\n",
      "|    103309| 3.947611710323575|\n",
      "|    103568|               2.0|\n",
      "|    103446|               3.0|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_avg_rating_df = final_df.groupBy(\"CustomerID\").agg(F.avg(\"Rate\").alias(\"average_rating\"))\n",
    "customer_avg_rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+-------------------+----------------+--------------------+---------+----------+----+\n",
      "|CustomerID|ProductID|Rate|         CreateDate|        NickName|                Name|UnitPrice|      Date|Hour|\n",
      "+----------+---------+----+-------------------+----------------+--------------------+---------+----------+----+\n",
      "|    103416|      619|   1|2018/01/01 01:36:30|    vanduong0403|Le Vian Chocolate...|     1049|2018-01-01|null|\n",
      "|    103654|      411|   1|2018/01/01 01:36:35| thutruc.huynh.3|Diamond Heart Nec...|   299.99|2018-01-01|null|\n",
      "|    103954|      298|   3|2018/01/01 01:36:38|    ha.n.hien.75|Heart Ring 1/5 ct...|      479|2018-01-01|null|\n",
      "|    103672|      361|   5|2018/01/01 01:37:15|   nam.kimcham.1|Neil Lane Diamond...|   719.99|2018-01-01|null|\n",
      "|    103960|      536|   5|2018/01/01 02:36:25|        tuanpkna|Disney Treasures ...|   449.99|2018-01-01|null|\n",
      "|    103372|      481|   2|2018/01/01 02:36:32|      damphuhanh|Amethyst Heart Ne...|   599.99|2018-01-01|null|\n",
      "|    103444|      132|   1|2018/01/01 02:36:34|    Kukumalu.Thu|        Chân Váy B&Y|        7|2018-01-01|null|\n",
      "|    103831|       41|   1|2018/01/01 02:36:41|           hip01|Flower Girl Bracelet|      360|2018-01-01|null|\n",
      "|    103541|      498|   5|2018/01/01 02:36:50|     kitty.ngo.5|Citrine & Diamond...|   179.99|2018-01-01|null|\n",
      "|    103819|      155|   4|2018/01/01 02:37:10|tran.anh.khoa.86|  Túi mây hình thang|       24|2018-01-01|null|\n",
      "+----------+---------+----+-------------------+----------------+--------------------+---------+----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CreateDate 컬럼을 날짜와 시간으로 분리\n",
    "final_df = final_df.withColumn(\"Date\", F.to_date(\"CreateDate\", \"yyyy/MM/dd HH:mm:ss\"))\n",
    "final_df = final_df.withColumn(\"Hour\", F.hour(\"CreateDate\"))\n",
    "\n",
    "final_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|          NickName|purchase_count|\n",
      "+------------------+--------------+\n",
      "|          chausoco|           158|\n",
      "|            Kojumi|           229|\n",
      "|        trannhatvy|           273|\n",
      "|          ruud0407|           368|\n",
      "|         lenam.uit|          1427|\n",
      "|        thanh.cule|           157|\n",
      "|        nguyettram|           274|\n",
      "|NguyenVuNhatChuong|           187|\n",
      "|           ptkiuee|           144|\n",
      "|     tuonglam.dang|          1088|\n",
      "|           thuattq|           406|\n",
      "|     dang.thang.31|           395|\n",
      "|          phan.nhu|           117|\n",
      "|            hhoahi|           135|\n",
      "|        hoaquachsd|           158|\n",
      "|         zeatop939|           289|\n",
      "|    huong.truongmy|           265|\n",
      "|          belimoon|           183|\n",
      "|            ac7ive|           100|\n",
      "|         daomandat|           746|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nickname_purchase_count_df = final_df.groupBy(\"NickName\").agg(F.count(\"ProductID\").alias(\"purchase_count\"))\n",
    "nickname_purchase_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                Name|     total_revenue|\n",
      "+--------------------+------------------+\n",
      "|Diamond Necklace ...|          125121.0|\n",
      "|     Dip Dye Sweater|            2924.0|\n",
      "|Le Vian Diamond R...|236598.17999999947|\n",
      "|Converse đen cổ thấp|            7986.0|\n",
      "|Flower Earrings 1...| 49246.37999999997|\n",
      "|Disney Treasures ...|               0.0|\n",
      "|  Vải Thiều Thanh Hà| 2187.009999999999|\n",
      "|Hoop Earrings 14K...| 37598.12000000007|\n",
      "|Unstoppable Love ...|522497.90999999864|\n",
      "|The Gangster, The...|             198.0|\n",
      "|     Science & Faith|               0.0|\n",
      "|         Xiaomi Mi 8|          114800.0|\n",
      "|Hallmark Diamonds...|23008.230000000058|\n",
      "|    Paper Coffee Cup|              92.5|\n",
      "|    Levi's 511 Jeans|            7438.5|\n",
      "|Basic Jogger In B...|            3580.0|\n",
      "|Disney Treasures ...|63358.079999999885|\n",
      "|Le Bleu Homestay ...| 5737.639999999994|\n",
      "|Samsung Galaxy Z ...|           67344.0|\n",
      "|Diamond Heart Nec...| 40597.97000000004|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_revenue_df = final_df.groupBy(\"Name\").agg(F.sum(\"UnitPrice\").alias(\"total_revenue\"))\n",
    "total_revenue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+-------------------+--------------------+--------------------+---------+\n",
      "|CustomerID|ProductID|Rate|         CreateDate|            NickName|                Name|UnitPrice|\n",
      "+----------+---------+----+-------------------+--------------------+--------------------+---------+\n",
      "|    103672|      361|   5|2018/01/01 01:37:15|       nam.kimcham.1|Neil Lane Diamond...|   719.99|\n",
      "|    103960|      536|   5|2018/01/01 02:36:25|            tuanpkna|Disney Treasures ...|   449.99|\n",
      "|    103541|      498|   5|2018/01/01 02:36:50|         kitty.ngo.5|Citrine & Diamond...|   179.99|\n",
      "|    103819|      155|   4|2018/01/01 02:37:10|    tran.anh.khoa.86|  Túi mây hình thang|       24|\n",
      "|    103806|      310|   4|2018/01/01 03:36:20|        trung.la.315|Le Vian Diamond R...|  1299.99|\n",
      "|    103726|      260|   4|2018/01/01 03:36:20|      phuoc.buithimy|Men's Diamond Ban...|  3999.99|\n",
      "|    103651|      388|   5|2018/01/01 03:36:57|   thanh.thao.581187|Garnet MOM Heart ...|   179.99|\n",
      "|    103629|      547|   4|2018/01/01 03:37:11|      selena.pham.90|Unstoppable Love ...|    79.99|\n",
      "|    103672|       52|   5|2018/01/01 03:37:30|       nam.kimcham.1|BLACKPINK 2ND MIN...|       30|\n",
      "|    103377|      468|   5|2018/01/01 03:37:53|           cskt.coop|\"Love\" Necklace 1...|   399.99|\n",
      "|    103911|      120|   5|2018/01/01 04:36:15|     macquanthungdit|            Quai vải|        2|\n",
      "|    103629|      634|   4|2018/01/01 04:37:11|      selena.pham.90|iPhone 13 Pro Max...|     1020|\n",
      "|    103544|       62|   5|2018/01/01 04:37:22|             vu.tiet|Ống hút gạo OCHAO...|        3|\n",
      "|    103641|      414|   4|2018/01/01 04:37:52|       tuonglam.dang|Children's Disney...|   199.99|\n",
      "|    103728|      112|   5|2018/01/01 05:36:56|     cathynguyethang|    New Banlance 580|       20|\n",
      "|    103676|      180|   4|2018/01/01 05:37:26|        hoangleo1989| Beverage Steeve Bag|        1|\n",
      "|    103545|       78|   5|2018/01/01 06:37:15|angelathanhthuy.tran|           Nhóm test|        2|\n",
      "|    103894|      215|   4|2018/01/01 07:36:11|             ptkiuee|Diamond Promise R...|      289|\n",
      "|    103651|      280|   5|2018/01/01 08:36:52|   thanh.thao.581187|Leo Zodiac Ring 1...|   149.99|\n",
      "|    103350|      368|   5|2018/01/01 08:36:57|          huanoppa89|Diamond Heart Nec...|       80|\n",
      "+----------+---------+----+-------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "high_rated_products = final_df.filter(final_df[\"Rate\"] >= 4)\n",
    "high_rated_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+-------------------+------------+--------------------+---------+\n",
      "|CustomerID|ProductID|Rate|         CreateDate|    NickName|                Name|UnitPrice|\n",
      "+----------+---------+----+-------------------+------------+--------------------+---------+\n",
      "|    103416|      619|   1|2018/01/01 01:36:30|vanduong0403|Le Vian Chocolate...|     1049|\n",
      "|    103416|       59|   3|2018/01/05 22:37:32|vanduong0403|Hành Trình Về Phư...|        2|\n",
      "|    103416|      170|   3|2018/01/07 04:37:32|vanduong0403|      Short Jeans 02|       18|\n",
      "|    103416|      551|   3|2018/01/09 12:37:32|vanduong0403|Unstoppable Love ...|   149.99|\n",
      "|    103416|      583|   3|2018/01/10 10:37:32|vanduong0403|Hoop Earrings 14K...|    46.99|\n",
      "|    103416|      429|   3|2018/01/23 08:37:32|vanduong0403|Lab-Created Diamo...|   799.99|\n",
      "|    103416|      648|   3|2018/01/24 02:37:32|vanduong0403|Diamond Promise R...|   299.99|\n",
      "|    103416|      184|   1|2018/01/26 14:36:30|vanduong0403|    Paper Coffee Cup|      0.5|\n",
      "|    103416|      460|   3|2018/01/27 16:37:32|vanduong0403|Pisces Zodiac Nec...|      329|\n",
      "|    103416|      509|   3|2018/01/28 07:37:32|vanduong0403|Encircled by Love...|   999.99|\n",
      "|    103416|      234|   3|2018/01/31 18:37:32|vanduong0403|3-Stone Promise R...|      279|\n",
      "|    103416|      337|   1|2018/02/01 12:36:30|vanduong0403|Tanzanite Ring St...|  2499.99|\n",
      "|    103416|       30|   2|2018/02/04 20:37:05|vanduong0403|    Levi's 511 Jeans|     43.5|\n",
      "|    103416|      611|   3|2018/02/06 18:37:32|vanduong0403|Le Vian Opal Earr...|   839.99|\n",
      "|    103416|      179|   3|2018/02/07 19:37:32|vanduong0403|Samsung Galaxy No...|      429|\n",
      "|    103416|      431|   2|2018/02/08 18:37:05|vanduong0403|Disney Treasures ...|   419.99|\n",
      "|    103416|      636|   1|2018/02/08 19:36:30|vanduong0403|Samsung Galaxy S2...|      536|\n",
      "|    103416|       38|   3|2018/02/09 04:37:32|vanduong0403|    First Prize Pies|       51|\n",
      "|    103416|       91|   2|2018/02/10 21:37:05|vanduong0403|Giày Sneaker Unis...|       20|\n",
      "|    103416|      344|   3|2018/02/13 15:37:32|vanduong0403|Stackable Diamond...|      179|\n",
      "+----------+---------+----+-------------------+------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 13:55:11 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 935011 ms exceeds timeout 120000 ms\n",
      "24/06/20 13:55:11 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# 특정 고객 ID의 데이터 필터링\n",
    "specific_customer_data = final_df.filter(final_df[\"CustomerID\"] == 103416)\n",
    "specific_customer_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
